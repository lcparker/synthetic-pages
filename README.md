# Description of the repository
This repository is home to the code I'm working on as part of the [Vesuvius Challenge](https://scrollprize.org).

Here, you will find:
* Algorithms to generate synthetic instance-labelled data scroll volume data. We've found this useful for training instance segmentation neural networks 
    - `bezier_volume_generation.py`
* PyTorch datasets that can be plugged into existing network approaches. Two PyTorch datasets are provided in `synthetic_pages.datasets`:
    - `InstanceCubesDataset`, which can be used to feed 3D scroll volumes extracted from the scroll CT
    - `SyntheticInstanceCubesDataset`, which generates *synthetic* pseudo-volumes. We have found these useful for pretraining our networks.
    - These are based on the ones from [Tim Skinner's progress prize submission](https://github.com/tspersonalgithub/december_2024_progress_submission)
* Scripts that can be used to download raw data (TIFs) of the scrolls, convert them to cubes, and infer them (this is useful for doing data processing on the cloud) (in `scripts/`)
* `label.py`, a volume annotation tool that I'm hoping to use for post-processing instance-segmentation predictions, stitching them into larger segments for flattening
* Code to match inferred labels on adjacent volumes, for generating larger sheets (in `match_stitches`)

# Why is this important?
To do ink detection well, you need lots of high-quality flattened portions of papyrus. To do that well, you probably need lots of high-quality data and a good neural network. Although data for this task is scarce and expensive to obtain, the overall structure is simple: small volumes of compressed sheets of papyrus. 

Sheets of papyrus can be artificially generated by creating non-intersecting [Bezier surfaces](https://en.wikipedia.org/wiki/B%C3%A9zier_surface) and using [signed distance functions](https://en.wikipedia.org/wiki/Signed_distance_function) to create masks. There is some evidence that this approach can work well in data-constrained situations (for an example, see the paper [deep leaf segmentation using synthetic data](https://arxiv.org/pdf/1807.10931)).

The code contained in this repository is a start on solving the synthetic pretraining problem.

# Installation and usage
```
conda create -n synthetic-pages python=3.10
conda activate synthetic-pages
python -m pip install -e . # In the root directory of the project
```

To generate a sample synthetic volume, use
```
python synthetic_pages/bezier_volume_deformation.py
```
which will save a synthetically generated labelmap in `labels.nrrd`.

To see how full synthetic volumes are generated, refer to `notebooks/howto-generate-synthetic-pages.ipynb`.

----
If you find it useful, we'd love to hear from you! You can contact `@lcparker` on the scrolls discord or via the email account used in these git commits.
